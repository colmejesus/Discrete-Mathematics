
## Time Complexity (Big O notation)

Time complexity is a way of measuring how long it takes an algorithm to run as the size of the input grows. The goal is to determine how much time an algorithm will take to solve a problem based on the size of the input, so that we can compare the efficiency of different algorithms.

In layman's terms, think of time complexity as a way to describe how long it takes to finish a task as the amount of work to be done increases. For example, if you are washing dishes, the time it takes you to wash a certain number of dishes is the time complexity of the task, and the number of dishes you have to wash is the size of the input.

In computer science, time complexity is expressed using Big O notation, which gives us an upper bound on the growth rate of the algorithm's running time as the size of the input grows. For example, an algorithm with a time complexity of O(n) means that the time it takes to run the algorithm grows linearly with the size of the input, which means that the larger the input, the more time it takes to run. On the other hand, an algorithm with a time complexity of O(n^2) means that the time it takes to run the algorithm grows as the square of the size of the input, which means that the larger the input, the much more time it takes to run.

There are three ways to analyze the time complexity of an algorithm: worst case, best case, and average case.

1.  Worst case time complexity: This is the maximum amount of time the algorithm takes to solve the problem for any input of a given size. It is the upper bound of the time complexity of an algorithm and is typically used to determine the maximum resources (such as time and memory) required to run the algorithm.
    
2.  Best case time complexity: This is the minimum amount of time the algorithm takes to solve the problem for any input of a given size. It is the lower bound of the time complexity of an algorithm and is typically used to determine the minimum resources required to run the algorithm.
    
3.  Average case time complexity: This is the expected amount of time the algorithm takes to solve the problem for any input of a given size. It takes into account all possible inputs of a given size and calculates the average time it takes to solve the problem. This is a more realistic measure of the time complexity of an algorithm and is often used to compare the efficiency of different algorithms.
    

Here's an example to help understand these concepts:

Suppose we have an algorithm to search for a specific number in a list of numbers. The worst-case time complexity of this algorithm would be O(n), where n is the number of elements in the list. This means that in the worst case, it would take the algorithm n steps to find the number, where n is the number of elements in the list. The best-case time complexity would be O(1), which means that in the best case, it would take the algorithm only 1 step to find the number. The average case time complexity would be somewhere between O(1) and O(n), depending on how often the number is found and where it is found in the list.

There are several common time complexities used in computer science to describe the efficiency of algorithms:

1.  O(1) - Constant time: The running time of the algorithm remains constant and does not change with the size of the input. Examples of algorithms with O(1) time complexity include accessing an element in an array using an index, or checking if a number is even or odd.
    
2.  O(log n) - Logarithmic time: The running time of the algorithm grows logarithmically with the size of the input. Examples of algorithms with O(log n) time complexity include binary search and quick sort.
    
3.  O(n) - Linear time: The running time of the algorithm grows linearly with the size of the input. Examples of algorithms with O(n) time complexity include sequential search and linear search.
    
4.  O(n log n) - Log-linear time: The running time of the algorithm grows as a logarithmic function of the size of the input multiplied by the size of the input. Examples of algorithms with O(n log n) time complexity include merge sort and heap sort.
    
5.  O(n^2) - Quadratic time: The running time of the algorithm grows as the square of the size of the input. Examples of algorithms with O(n^2) time complexity include bubble sort and insertion sort.
    
6.  O(2^n) - Exponential time: The running time of the algorithm grows exponentially with the size of the input. Examples of algorithms with O(2^n) time complexity include brute-force solutions for problems such as the traveling salesman problem.
    

Here's a Graphical representation of some of the time complexities mentioned above:

![big_o_notations](https://user-images.githubusercontent.com/124640512/218214511-c080c30f-bc88-4b87-9e0a-a1313242f261.png)


These are the most common time complexities, but there are others as well. The time complexity of an algorithm can often be optimized by using a more efficient algorithm, data structure, or by making changes to the implementation of the algorithm.

## Space Complexity:

Space complexity refers to the amount of memory used by an algorithm. It's a measure of the amount of memory used by an algorithm during its execution. The space complexity of an algorithm can be expressed as a function of the size of the input, just like time complexity.

For example, consider a simple algorithm that adds up all the numbers in an array. The space complexity of this algorithm is O(1), because it uses a single variable to store the sum, and the size of the variable does not change as the size of the input grows.

Now, consider an algorithm that sorts an array of numbers. This algorithm might use an additional array to store the sorted numbers, and the size of this additional array will be the same as the size of the input. The space complexity of this algorithm is O(n), because the size of the additional array grows linearly with the size of the input.

Another example is a recursive algorithm, such as the fibonacci sequence. Each time the function is called, a new set of variables is added to the call stack, causing the memory usage to grow with each iteration. The space complexity of this algorithm is O(n), because the amount of memory used grows linearly with the size of the input.

In general, it is important to consider both the time and space complexity of an algorithm when choosing an algorithm to solve a particular problem. An algorithm with a good time complexity might use a lot of memory, making it unsuitable for use on large datasets, while an algorithm with a good space complexity might be slow, making it unsuitable for real-time applications.
